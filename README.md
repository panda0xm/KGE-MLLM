# KGE-MLLM
Knowledge-Graph Enhanced Multi-modal Language Modal

The basic code is from LLaVA: https://github.com/haotian-liu/LLaVA/

The pre-trained model such as LLaVA, vicuna,clip  comes from the Internet.
The training data such as scienceqa, mmmu, mmmu-pro comes from the Internet.
Some directory in the project is empty, they just show the framework of the work, the whole project over 700GB. 

Based on this, we trained an independent reasoning model, achieving KGE-MLLM
