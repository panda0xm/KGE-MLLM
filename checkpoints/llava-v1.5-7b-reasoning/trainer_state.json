{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 317,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0031545741324921135,
      "grad_norm": 4.991551019218552,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.5449,
      "step": 1
    },
    {
      "epoch": 0.006309148264984227,
      "grad_norm": 5.171180758422274,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.5442,
      "step": 2
    },
    {
      "epoch": 0.00946372239747634,
      "grad_norm": 5.052398348985834,
      "learning_rate": 6e-06,
      "loss": 1.5358,
      "step": 3
    },
    {
      "epoch": 0.012618296529968454,
      "grad_norm": 4.8258453390927185,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.546,
      "step": 4
    },
    {
      "epoch": 0.015772870662460567,
      "grad_norm": 3.404006877807936,
      "learning_rate": 1e-05,
      "loss": 1.429,
      "step": 5
    },
    {
      "epoch": 0.01892744479495268,
      "grad_norm": 2.527689264252198,
      "learning_rate": 1.2e-05,
      "loss": 1.2394,
      "step": 6
    },
    {
      "epoch": 0.022082018927444796,
      "grad_norm": 2.336784864671872,
      "learning_rate": 1.4e-05,
      "loss": 1.2515,
      "step": 7
    },
    {
      "epoch": 0.025236593059936908,
      "grad_norm": 2.880703421676924,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.1475,
      "step": 8
    },
    {
      "epoch": 0.028391167192429023,
      "grad_norm": 2.1423772370577194,
      "learning_rate": 1.8e-05,
      "loss": 1.1021,
      "step": 9
    },
    {
      "epoch": 0.031545741324921134,
      "grad_norm": 1.884827962023935,
      "learning_rate": 2e-05,
      "loss": 1.0481,
      "step": 10
    },
    {
      "epoch": 0.03470031545741325,
      "grad_norm": 1.8682232509670809,
      "learning_rate": 1.99994764125734e-05,
      "loss": 0.9837,
      "step": 11
    },
    {
      "epoch": 0.03785488958990536,
      "grad_norm": 1.3934987145701894,
      "learning_rate": 1.9997905705122352e-05,
      "loss": 0.9436,
      "step": 12
    },
    {
      "epoch": 0.04100946372239748,
      "grad_norm": 1.3806656518361384,
      "learning_rate": 1.9995288042127396e-05,
      "loss": 0.9227,
      "step": 13
    },
    {
      "epoch": 0.04416403785488959,
      "grad_norm": 1.224630999783706,
      "learning_rate": 1.9991623697703613e-05,
      "loss": 0.8741,
      "step": 14
    },
    {
      "epoch": 0.0473186119873817,
      "grad_norm": 1.2555095928549982,
      "learning_rate": 1.998691305557194e-05,
      "loss": 0.8616,
      "step": 15
    },
    {
      "epoch": 0.050473186119873815,
      "grad_norm": 1.1863520914969352,
      "learning_rate": 1.9981156609018977e-05,
      "loss": 0.8678,
      "step": 16
    },
    {
      "epoch": 0.05362776025236593,
      "grad_norm": 1.0606235274364912,
      "learning_rate": 1.9974354960845326e-05,
      "loss": 0.8518,
      "step": 17
    },
    {
      "epoch": 0.056782334384858045,
      "grad_norm": 0.9946509031139025,
      "learning_rate": 1.9966508823302484e-05,
      "loss": 0.8755,
      "step": 18
    },
    {
      "epoch": 0.05993690851735016,
      "grad_norm": 1.0654383384647574,
      "learning_rate": 1.9957619018018243e-05,
      "loss": 0.8031,
      "step": 19
    },
    {
      "epoch": 0.06309148264984227,
      "grad_norm": 0.9844728182262209,
      "learning_rate": 1.9947686475910656e-05,
      "loss": 0.7822,
      "step": 20
    },
    {
      "epoch": 0.06624605678233439,
      "grad_norm": 1.0351748414568003,
      "learning_rate": 1.9936712237090554e-05,
      "loss": 0.7497,
      "step": 21
    },
    {
      "epoch": 0.0694006309148265,
      "grad_norm": 1.0335655049252874,
      "learning_rate": 1.9924697450752636e-05,
      "loss": 0.8097,
      "step": 22
    },
    {
      "epoch": 0.07255520504731862,
      "grad_norm": 0.9455888837658566,
      "learning_rate": 1.991164337505511e-05,
      "loss": 0.7736,
      "step": 23
    },
    {
      "epoch": 0.07570977917981073,
      "grad_norm": 1.0016359130550274,
      "learning_rate": 1.9897551376987948e-05,
      "loss": 0.7582,
      "step": 24
    },
    {
      "epoch": 0.07886435331230283,
      "grad_norm": 0.959743526647615,
      "learning_rate": 1.9882422932229765e-05,
      "loss": 0.7848,
      "step": 25
    },
    {
      "epoch": 0.08201892744479496,
      "grad_norm": 0.9902160114687725,
      "learning_rate": 1.9866259624993246e-05,
      "loss": 0.7646,
      "step": 26
    },
    {
      "epoch": 0.08517350157728706,
      "grad_norm": 0.9434207305942572,
      "learning_rate": 1.9849063147859282e-05,
      "loss": 0.7117,
      "step": 27
    },
    {
      "epoch": 0.08832807570977919,
      "grad_norm": 0.9671611754387754,
      "learning_rate": 1.983083530159971e-05,
      "loss": 0.7007,
      "step": 28
    },
    {
      "epoch": 0.0914826498422713,
      "grad_norm": 0.8737036004402091,
      "learning_rate": 1.9811577994988755e-05,
      "loss": 0.6896,
      "step": 29
    },
    {
      "epoch": 0.0946372239747634,
      "grad_norm": 0.874879373825322,
      "learning_rate": 1.979129324460314e-05,
      "loss": 0.7271,
      "step": 30
    },
    {
      "epoch": 0.09779179810725552,
      "grad_norm": 0.8749766867810754,
      "learning_rate": 1.9769983174610918e-05,
      "loss": 0.6942,
      "step": 31
    },
    {
      "epoch": 0.10094637223974763,
      "grad_norm": 0.9419123224292612,
      "learning_rate": 1.974765001654903e-05,
      "loss": 0.7176,
      "step": 32
    },
    {
      "epoch": 0.10410094637223975,
      "grad_norm": 0.9821047615229729,
      "learning_rate": 1.9724296109089623e-05,
      "loss": 0.7531,
      "step": 33
    },
    {
      "epoch": 0.10725552050473186,
      "grad_norm": 0.9459259658311194,
      "learning_rate": 1.9699923897795165e-05,
      "loss": 0.7102,
      "step": 34
    },
    {
      "epoch": 0.11041009463722397,
      "grad_norm": 0.9512677023106465,
      "learning_rate": 1.9674535934862327e-05,
      "loss": 0.7253,
      "step": 35
    },
    {
      "epoch": 0.11356466876971609,
      "grad_norm": 0.9148597445035768,
      "learning_rate": 1.9648134878854747e-05,
      "loss": 0.7257,
      "step": 36
    },
    {
      "epoch": 0.1167192429022082,
      "grad_norm": 0.9071246359326176,
      "learning_rate": 1.9620723494424627e-05,
      "loss": 0.6861,
      "step": 37
    },
    {
      "epoch": 0.11987381703470032,
      "grad_norm": 0.9235635586425757,
      "learning_rate": 1.9592304652023208e-05,
      "loss": 0.7204,
      "step": 38
    },
    {
      "epoch": 0.12302839116719243,
      "grad_norm": 0.9351838428645597,
      "learning_rate": 1.9562881327600197e-05,
      "loss": 0.6926,
      "step": 39
    },
    {
      "epoch": 0.12618296529968454,
      "grad_norm": 0.8675726633960811,
      "learning_rate": 1.9532456602292148e-05,
      "loss": 0.6853,
      "step": 40
    },
    {
      "epoch": 0.12933753943217666,
      "grad_norm": 0.8651838363258774,
      "learning_rate": 1.950103366209978e-05,
      "loss": 0.7301,
      "step": 41
    },
    {
      "epoch": 0.13249211356466878,
      "grad_norm": 0.8442902716159056,
      "learning_rate": 1.9468615797554374e-05,
      "loss": 0.6914,
      "step": 42
    },
    {
      "epoch": 0.13564668769716087,
      "grad_norm": 0.8425051905503441,
      "learning_rate": 1.943520640337318e-05,
      "loss": 0.6781,
      "step": 43
    },
    {
      "epoch": 0.138801261829653,
      "grad_norm": 0.8447480508658269,
      "learning_rate": 1.9400808978103948e-05,
      "loss": 0.698,
      "step": 44
    },
    {
      "epoch": 0.14195583596214512,
      "grad_norm": 0.8882452502339697,
      "learning_rate": 1.936542712375855e-05,
      "loss": 0.7028,
      "step": 45
    },
    {
      "epoch": 0.14511041009463724,
      "grad_norm": 0.8673915192062216,
      "learning_rate": 1.9329064545435803e-05,
      "loss": 0.6969,
      "step": 46
    },
    {
      "epoch": 0.14826498422712933,
      "grad_norm": 0.9706693804924906,
      "learning_rate": 1.929172505093347e-05,
      "loss": 0.6814,
      "step": 47
    },
    {
      "epoch": 0.15141955835962145,
      "grad_norm": 0.869312890861048,
      "learning_rate": 1.9253412550349507e-05,
      "loss": 0.696,
      "step": 48
    },
    {
      "epoch": 0.15457413249211358,
      "grad_norm": 0.8671910841862964,
      "learning_rate": 1.9214131055672648e-05,
      "loss": 0.6695,
      "step": 49
    },
    {
      "epoch": 0.15772870662460567,
      "grad_norm": 0.8782701621758165,
      "learning_rate": 1.917388468036222e-05,
      "loss": 0.655,
      "step": 50
    },
    {
      "epoch": 0.1608832807570978,
      "grad_norm": 0.9220357672433334,
      "learning_rate": 1.913267763891745e-05,
      "loss": 0.7005,
      "step": 51
    },
    {
      "epoch": 0.1640378548895899,
      "grad_norm": 0.8702922844089401,
      "learning_rate": 1.9090514246436085e-05,
      "loss": 0.6987,
      "step": 52
    },
    {
      "epoch": 0.167192429022082,
      "grad_norm": 0.8579466706447327,
      "learning_rate": 1.904739891816257e-05,
      "loss": 0.6078,
      "step": 53
    },
    {
      "epoch": 0.17034700315457413,
      "grad_norm": 0.8893164208870957,
      "learning_rate": 1.9003336169025655e-05,
      "loss": 0.6815,
      "step": 54
    },
    {
      "epoch": 0.17350157728706625,
      "grad_norm": 0.9019392724513261,
      "learning_rate": 1.8958330613165622e-05,
      "loss": 0.7043,
      "step": 55
    },
    {
      "epoch": 0.17665615141955837,
      "grad_norm": 0.9357513829955775,
      "learning_rate": 1.891238696345111e-05,
      "loss": 0.6681,
      "step": 56
    },
    {
      "epoch": 0.17981072555205047,
      "grad_norm": 0.8470326225656954,
      "learning_rate": 1.8865510030985588e-05,
      "loss": 0.6637,
      "step": 57
    },
    {
      "epoch": 0.1829652996845426,
      "grad_norm": 0.8465656699591587,
      "learning_rate": 1.8817704724603536e-05,
      "loss": 0.6745,
      "step": 58
    },
    {
      "epoch": 0.1861198738170347,
      "grad_norm": 0.8704059619009971,
      "learning_rate": 1.8768976050356428e-05,
      "loss": 0.6151,
      "step": 59
    },
    {
      "epoch": 0.1892744479495268,
      "grad_norm": 0.8621568384104421,
      "learning_rate": 1.8719329110988487e-05,
      "loss": 0.6878,
      "step": 60
    },
    {
      "epoch": 0.19242902208201892,
      "grad_norm": 0.8033863143078525,
      "learning_rate": 1.8668769105402366e-05,
      "loss": 0.5859,
      "step": 61
    },
    {
      "epoch": 0.19558359621451105,
      "grad_norm": 0.8566338157555143,
      "learning_rate": 1.8617301328114704e-05,
      "loss": 0.6478,
      "step": 62
    },
    {
      "epoch": 0.19873817034700317,
      "grad_norm": 0.8550915463026342,
      "learning_rate": 1.8564931168701713e-05,
      "loss": 0.6944,
      "step": 63
    },
    {
      "epoch": 0.20189274447949526,
      "grad_norm": 0.8394416055891806,
      "learning_rate": 1.85116641112348e-05,
      "loss": 0.6472,
      "step": 64
    },
    {
      "epoch": 0.20504731861198738,
      "grad_norm": 0.9368935472880039,
      "learning_rate": 1.845750573370626e-05,
      "loss": 0.6459,
      "step": 65
    },
    {
      "epoch": 0.2082018927444795,
      "grad_norm": 0.8901137631532636,
      "learning_rate": 1.8402461707445206e-05,
      "loss": 0.6543,
      "step": 66
    },
    {
      "epoch": 0.2113564668769716,
      "grad_norm": 0.8023738946071911,
      "learning_rate": 1.8346537796523643e-05,
      "loss": 0.6538,
      "step": 67
    },
    {
      "epoch": 0.21451104100946372,
      "grad_norm": 0.903505114790026,
      "learning_rate": 1.8289739857152903e-05,
      "loss": 0.6362,
      "step": 68
    },
    {
      "epoch": 0.21766561514195584,
      "grad_norm": 0.922029657341168,
      "learning_rate": 1.823207383707036e-05,
      "loss": 0.6385,
      "step": 69
    },
    {
      "epoch": 0.22082018927444794,
      "grad_norm": 0.8422698824554644,
      "learning_rate": 1.8173545774916628e-05,
      "loss": 0.6398,
      "step": 70
    },
    {
      "epoch": 0.22397476340694006,
      "grad_norm": 0.9471459470054127,
      "learning_rate": 1.8114161799603195e-05,
      "loss": 0.6381,
      "step": 71
    },
    {
      "epoch": 0.22712933753943218,
      "grad_norm": 0.8747885933687363,
      "learning_rate": 1.8053928129670624e-05,
      "loss": 0.6427,
      "step": 72
    },
    {
      "epoch": 0.2302839116719243,
      "grad_norm": 0.8731869430555832,
      "learning_rate": 1.7992851072637366e-05,
      "loss": 0.6322,
      "step": 73
    },
    {
      "epoch": 0.2334384858044164,
      "grad_norm": 0.8959722096765851,
      "learning_rate": 1.793093702433924e-05,
      "loss": 0.6264,
      "step": 74
    },
    {
      "epoch": 0.23659305993690852,
      "grad_norm": 0.8497188034220844,
      "learning_rate": 1.7868192468259686e-05,
      "loss": 0.6553,
      "step": 75
    },
    {
      "epoch": 0.23974763406940064,
      "grad_norm": 0.8699375556365588,
      "learning_rate": 1.7804623974850844e-05,
      "loss": 0.6305,
      "step": 76
    },
    {
      "epoch": 0.24290220820189273,
      "grad_norm": 0.8450266796090004,
      "learning_rate": 1.7740238200845485e-05,
      "loss": 0.6092,
      "step": 77
    },
    {
      "epoch": 0.24605678233438485,
      "grad_norm": 0.8868416423578316,
      "learning_rate": 1.7675041888559952e-05,
      "loss": 0.6739,
      "step": 78
    },
    {
      "epoch": 0.24921135646687698,
      "grad_norm": 0.8567691137755006,
      "learning_rate": 1.7609041865188122e-05,
      "loss": 0.6185,
      "step": 79
    },
    {
      "epoch": 0.25236593059936907,
      "grad_norm": 0.8839696115825323,
      "learning_rate": 1.754224504208647e-05,
      "loss": 0.6127,
      "step": 80
    },
    {
      "epoch": 0.2555205047318612,
      "grad_norm": 0.9034266312323332,
      "learning_rate": 1.7474658414050344e-05,
      "loss": 0.6552,
      "step": 81
    },
    {
      "epoch": 0.2586750788643533,
      "grad_norm": 0.9218686025275764,
      "learning_rate": 1.7406289058581466e-05,
      "loss": 0.6575,
      "step": 82
    },
    {
      "epoch": 0.2618296529968454,
      "grad_norm": 0.9030131646228647,
      "learning_rate": 1.7337144135146818e-05,
      "loss": 0.6567,
      "step": 83
    },
    {
      "epoch": 0.26498422712933756,
      "grad_norm": 0.844542017343332,
      "learning_rate": 1.7267230884428905e-05,
      "loss": 0.6253,
      "step": 84
    },
    {
      "epoch": 0.26813880126182965,
      "grad_norm": 0.9256863648965401,
      "learning_rate": 1.719655662756753e-05,
      "loss": 0.666,
      "step": 85
    },
    {
      "epoch": 0.27129337539432175,
      "grad_norm": 0.8769758594339074,
      "learning_rate": 1.7125128765393157e-05,
      "loss": 0.6605,
      "step": 86
    },
    {
      "epoch": 0.2744479495268139,
      "grad_norm": 0.9195307314332682,
      "learning_rate": 1.705295477765188e-05,
      "loss": 0.6633,
      "step": 87
    },
    {
      "epoch": 0.277602523659306,
      "grad_norm": 1.003662571954806,
      "learning_rate": 1.6980042222222216e-05,
      "loss": 0.6361,
      "step": 88
    },
    {
      "epoch": 0.2807570977917981,
      "grad_norm": 1.03401429672994,
      "learning_rate": 1.690639873432361e-05,
      "loss": 0.6824,
      "step": 89
    },
    {
      "epoch": 0.28391167192429023,
      "grad_norm": 0.9120497236080171,
      "learning_rate": 1.683203202571692e-05,
      "loss": 0.6844,
      "step": 90
    },
    {
      "epoch": 0.2870662460567823,
      "grad_norm": 0.9355673965598479,
      "learning_rate": 1.6756949883896874e-05,
      "loss": 0.6436,
      "step": 91
    },
    {
      "epoch": 0.2902208201892745,
      "grad_norm": 0.9279551776150905,
      "learning_rate": 1.668116017127655e-05,
      "loss": 0.6231,
      "step": 92
    },
    {
      "epoch": 0.29337539432176657,
      "grad_norm": 0.8595984852487247,
      "learning_rate": 1.6604670824364067e-05,
      "loss": 0.6581,
      "step": 93
    },
    {
      "epoch": 0.29652996845425866,
      "grad_norm": 0.9371432624685408,
      "learning_rate": 1.652748985293149e-05,
      "loss": 0.6111,
      "step": 94
    },
    {
      "epoch": 0.2996845425867508,
      "grad_norm": 0.8945424107819544,
      "learning_rate": 1.6449625339176056e-05,
      "loss": 0.6238,
      "step": 95
    },
    {
      "epoch": 0.3028391167192429,
      "grad_norm": 0.8159101886520962,
      "learning_rate": 1.6371085436873847e-05,
      "loss": 0.5716,
      "step": 96
    },
    {
      "epoch": 0.305993690851735,
      "grad_norm": 0.9838763686510698,
      "learning_rate": 1.6291878370525925e-05,
      "loss": 0.636,
      "step": 97
    },
    {
      "epoch": 0.30914826498422715,
      "grad_norm": 0.902966787300876,
      "learning_rate": 1.6212012434497103e-05,
      "loss": 0.6266,
      "step": 98
    },
    {
      "epoch": 0.31230283911671924,
      "grad_norm": 0.8475624077693824,
      "learning_rate": 1.6131495992147363e-05,
      "loss": 0.5835,
      "step": 99
    },
    {
      "epoch": 0.31545741324921134,
      "grad_norm": 0.8778636206060503,
      "learning_rate": 1.605033747495607e-05,
      "loss": 0.6091,
      "step": 100
    },
    {
      "epoch": 0.3186119873817035,
      "grad_norm": 0.902497833647063,
      "learning_rate": 1.596854538163906e-05,
      "loss": 0.6024,
      "step": 101
    },
    {
      "epoch": 0.3217665615141956,
      "grad_norm": 0.834407792867163,
      "learning_rate": 1.5886128277258665e-05,
      "loss": 0.6103,
      "step": 102
    },
    {
      "epoch": 0.3249211356466877,
      "grad_norm": 0.8549372808315743,
      "learning_rate": 1.58030947923268e-05,
      "loss": 0.6107,
      "step": 103
    },
    {
      "epoch": 0.3280757097791798,
      "grad_norm": 0.9240814581403706,
      "learning_rate": 1.571945362190121e-05,
      "loss": 0.6634,
      "step": 104
    },
    {
      "epoch": 0.3312302839116719,
      "grad_norm": 0.8370209373268286,
      "learning_rate": 1.563521352467493e-05,
      "loss": 0.5974,
      "step": 105
    },
    {
      "epoch": 0.334384858044164,
      "grad_norm": 0.8997958016362174,
      "learning_rate": 1.55503833220591e-05,
      "loss": 0.6442,
      "step": 106
    },
    {
      "epoch": 0.33753943217665616,
      "grad_norm": 0.9143517127117635,
      "learning_rate": 1.546497189725922e-05,
      "loss": 0.6311,
      "step": 107
    },
    {
      "epoch": 0.34069400630914826,
      "grad_norm": 0.8589953845849042,
      "learning_rate": 1.5378988194344913e-05,
      "loss": 0.6681,
      "step": 108
    },
    {
      "epoch": 0.3438485804416404,
      "grad_norm": 0.9304871161123862,
      "learning_rate": 1.5292441217313324e-05,
      "loss": 0.6454,
      "step": 109
    },
    {
      "epoch": 0.3470031545741325,
      "grad_norm": 0.9075292650431247,
      "learning_rate": 1.5205340029146256e-05,
      "loss": 0.6191,
      "step": 110
    },
    {
      "epoch": 0.3501577287066246,
      "grad_norm": 0.8866342188507438,
      "learning_rate": 1.5117693750861096e-05,
      "loss": 0.6309,
      "step": 111
    },
    {
      "epoch": 0.35331230283911674,
      "grad_norm": 0.9365408353356297,
      "learning_rate": 1.5029511560555707e-05,
      "loss": 0.6019,
      "step": 112
    },
    {
      "epoch": 0.35646687697160884,
      "grad_norm": 0.8743858650922481,
      "learning_rate": 1.4940802692447306e-05,
      "loss": 0.614,
      "step": 113
    },
    {
      "epoch": 0.35962145110410093,
      "grad_norm": 0.8721681087564115,
      "learning_rate": 1.4851576435905489e-05,
      "loss": 0.623,
      "step": 114
    },
    {
      "epoch": 0.3627760252365931,
      "grad_norm": 0.9037338110085954,
      "learning_rate": 1.4761842134479463e-05,
      "loss": 0.6484,
      "step": 115
    },
    {
      "epoch": 0.3659305993690852,
      "grad_norm": 0.8403059105336902,
      "learning_rate": 1.4671609184919622e-05,
      "loss": 0.53,
      "step": 116
    },
    {
      "epoch": 0.36908517350157727,
      "grad_norm": 0.8353989976801615,
      "learning_rate": 1.4580887036193539e-05,
      "loss": 0.5462,
      "step": 117
    },
    {
      "epoch": 0.3722397476340694,
      "grad_norm": 0.8864333350520177,
      "learning_rate": 1.4489685188496488e-05,
      "loss": 0.5773,
      "step": 118
    },
    {
      "epoch": 0.3753943217665615,
      "grad_norm": 0.8928125791356667,
      "learning_rate": 1.4398013192256615e-05,
      "loss": 0.5996,
      "step": 119
    },
    {
      "epoch": 0.3785488958990536,
      "grad_norm": 0.9440485497264509,
      "learning_rate": 1.4305880647134847e-05,
      "loss": 0.6079,
      "step": 120
    },
    {
      "epoch": 0.38170347003154576,
      "grad_norm": 0.8764760740842126,
      "learning_rate": 1.4213297201019618e-05,
      "loss": 0.5928,
      "step": 121
    },
    {
      "epoch": 0.38485804416403785,
      "grad_norm": 0.9046656878086043,
      "learning_rate": 1.4120272549016591e-05,
      "loss": 0.6147,
      "step": 122
    },
    {
      "epoch": 0.38801261829652994,
      "grad_norm": 0.8824838195383237,
      "learning_rate": 1.40268164324334e-05,
      "loss": 0.6025,
      "step": 123
    },
    {
      "epoch": 0.3911671924290221,
      "grad_norm": 0.7965569284860019,
      "learning_rate": 1.3932938637759555e-05,
      "loss": 0.6196,
      "step": 124
    },
    {
      "epoch": 0.3943217665615142,
      "grad_norm": 0.878224178992783,
      "learning_rate": 1.3838648995641645e-05,
      "loss": 0.589,
      "step": 125
    },
    {
      "epoch": 0.39747634069400634,
      "grad_norm": 0.9016950929983897,
      "learning_rate": 1.3743957379853885e-05,
      "loss": 0.6468,
      "step": 126
    },
    {
      "epoch": 0.40063091482649843,
      "grad_norm": 0.8786925208001064,
      "learning_rate": 1.3648873706264159e-05,
      "loss": 0.6176,
      "step": 127
    },
    {
      "epoch": 0.4037854889589905,
      "grad_norm": 0.8683234002656758,
      "learning_rate": 1.3553407931795662e-05,
      "loss": 0.6768,
      "step": 128
    },
    {
      "epoch": 0.4069400630914827,
      "grad_norm": 0.9405796442139681,
      "learning_rate": 1.3457570053384225e-05,
      "loss": 0.6268,
      "step": 129
    },
    {
      "epoch": 0.41009463722397477,
      "grad_norm": 0.885159405434453,
      "learning_rate": 1.3361370106931486e-05,
      "loss": 0.6318,
      "step": 130
    },
    {
      "epoch": 0.41324921135646686,
      "grad_norm": 0.8143364856132449,
      "learning_rate": 1.3264818166253917e-05,
      "loss": 0.5779,
      "step": 131
    },
    {
      "epoch": 0.416403785488959,
      "grad_norm": 0.8732971999638504,
      "learning_rate": 1.3167924342027947e-05,
      "loss": 0.5961,
      "step": 132
    },
    {
      "epoch": 0.4195583596214511,
      "grad_norm": 0.9233188198870517,
      "learning_rate": 1.3070698780731194e-05,
      "loss": 0.652,
      "step": 133
    },
    {
      "epoch": 0.4227129337539432,
      "grad_norm": 0.8944043815148699,
      "learning_rate": 1.2973151663579948e-05,
      "loss": 0.633,
      "step": 134
    },
    {
      "epoch": 0.42586750788643535,
      "grad_norm": 0.8752304522224081,
      "learning_rate": 1.2875293205463018e-05,
      "loss": 0.5856,
      "step": 135
    },
    {
      "epoch": 0.42902208201892744,
      "grad_norm": 0.8428835705574351,
      "learning_rate": 1.277713365387205e-05,
      "loss": 0.6123,
      "step": 136
    },
    {
      "epoch": 0.43217665615141954,
      "grad_norm": 0.8354617447536336,
      "learning_rate": 1.2678683287828451e-05,
      "loss": 0.5751,
      "step": 137
    },
    {
      "epoch": 0.4353312302839117,
      "grad_norm": 0.8587478107757187,
      "learning_rate": 1.257995241680698e-05,
      "loss": 0.5987,
      "step": 138
    },
    {
      "epoch": 0.4384858044164038,
      "grad_norm": 0.8974879170634598,
      "learning_rate": 1.2480951379656175e-05,
      "loss": 0.547,
      "step": 139
    },
    {
      "epoch": 0.4416403785488959,
      "grad_norm": 0.8616886581063007,
      "learning_rate": 1.2381690543515692e-05,
      "loss": 0.6015,
      "step": 140
    },
    {
      "epoch": 0.444794952681388,
      "grad_norm": 0.8937932799878165,
      "learning_rate": 1.2282180302730683e-05,
      "loss": 0.6182,
      "step": 141
    },
    {
      "epoch": 0.4479495268138801,
      "grad_norm": 0.8573360593259032,
      "learning_rate": 1.2182431077763317e-05,
      "loss": 0.5524,
      "step": 142
    },
    {
      "epoch": 0.45110410094637227,
      "grad_norm": 0.8740222883276443,
      "learning_rate": 1.2082453314101607e-05,
      "loss": 0.64,
      "step": 143
    },
    {
      "epoch": 0.45425867507886436,
      "grad_norm": 0.8971919289108966,
      "learning_rate": 1.1982257481165547e-05,
      "loss": 0.6066,
      "step": 144
    },
    {
      "epoch": 0.45741324921135645,
      "grad_norm": 0.8466551994055512,
      "learning_rate": 1.1881854071210805e-05,
      "loss": 0.5864,
      "step": 145
    },
    {
      "epoch": 0.4605678233438486,
      "grad_norm": 0.8798627095689837,
      "learning_rate": 1.1781253598229982e-05,
      "loss": 0.6417,
      "step": 146
    },
    {
      "epoch": 0.4637223974763407,
      "grad_norm": 0.8520318880504439,
      "learning_rate": 1.1680466596851635e-05,
      "loss": 0.5997,
      "step": 147
    },
    {
      "epoch": 0.4668769716088328,
      "grad_norm": 0.7992590049502128,
      "learning_rate": 1.1579503621237102e-05,
      "loss": 0.6302,
      "step": 148
    },
    {
      "epoch": 0.47003154574132494,
      "grad_norm": 0.8517311845077254,
      "learning_rate": 1.1478375243975298e-05,
      "loss": 0.571,
      "step": 149
    },
    {
      "epoch": 0.47318611987381703,
      "grad_norm": 0.8528739898102594,
      "learning_rate": 1.1377092054975586e-05,
      "loss": 0.5822,
      "step": 150
    },
    {
      "epoch": 0.47634069400630913,
      "grad_norm": 0.862597326637033,
      "learning_rate": 1.1275664660358818e-05,
      "loss": 0.6029,
      "step": 151
    },
    {
      "epoch": 0.4794952681388013,
      "grad_norm": 0.8276452507228796,
      "learning_rate": 1.1174103681346711e-05,
      "loss": 0.6121,
      "step": 152
    },
    {
      "epoch": 0.48264984227129337,
      "grad_norm": 0.831102032794066,
      "learning_rate": 1.1072419753149585e-05,
      "loss": 0.5834,
      "step": 153
    },
    {
      "epoch": 0.48580441640378547,
      "grad_norm": 0.8625400496564916,
      "learning_rate": 1.0970623523852699e-05,
      "loss": 0.6165,
      "step": 154
    },
    {
      "epoch": 0.4889589905362776,
      "grad_norm": 0.9210896922518162,
      "learning_rate": 1.0868725653301206e-05,
      "loss": 0.5847,
      "step": 155
    },
    {
      "epoch": 0.4921135646687697,
      "grad_norm": 0.8171045165050251,
      "learning_rate": 1.0766736811983864e-05,
      "loss": 0.5482,
      "step": 156
    },
    {
      "epoch": 0.4952681388012618,
      "grad_norm": 0.8458387202954412,
      "learning_rate": 1.066466767991567e-05,
      "loss": 0.595,
      "step": 157
    },
    {
      "epoch": 0.49842271293375395,
      "grad_norm": 0.9303253340673677,
      "learning_rate": 1.0562528945519463e-05,
      "loss": 0.602,
      "step": 158
    },
    {
      "epoch": 0.501577287066246,
      "grad_norm": 1.0013786134177682,
      "learning_rate": 1.0460331304506658e-05,
      "loss": 0.5878,
      "step": 159
    },
    {
      "epoch": 0.5047318611987381,
      "grad_norm": 0.8641452474633743,
      "learning_rate": 1.0358085458757233e-05,
      "loss": 0.6178,
      "step": 160
    },
    {
      "epoch": 0.5078864353312302,
      "grad_norm": 0.8672207981965124,
      "learning_rate": 1.0255802115199034e-05,
      "loss": 0.608,
      "step": 161
    },
    {
      "epoch": 0.5110410094637224,
      "grad_norm": 0.924581329048835,
      "learning_rate": 1.0153491984686595e-05,
      "loss": 0.6449,
      "step": 162
    },
    {
      "epoch": 0.5141955835962145,
      "grad_norm": 0.8583189895764876,
      "learning_rate": 1.0051165780879503e-05,
      "loss": 0.5621,
      "step": 163
    },
    {
      "epoch": 0.5173501577287066,
      "grad_norm": 0.8179698190334622,
      "learning_rate": 9.9488342191205e-06,
      "loss": 0.5914,
      "step": 164
    },
    {
      "epoch": 0.5205047318611987,
      "grad_norm": 0.9146700885144573,
      "learning_rate": 9.846508015313407e-06,
      "loss": 0.5927,
      "step": 165
    },
    {
      "epoch": 0.5236593059936908,
      "grad_norm": 0.9035482476491966,
      "learning_rate": 9.744197884800968e-06,
      "loss": 0.6347,
      "step": 166
    },
    {
      "epoch": 0.526813880126183,
      "grad_norm": 0.9106208495238672,
      "learning_rate": 9.64191454124277e-06,
      "loss": 0.5952,
      "step": 167
    },
    {
      "epoch": 0.5299684542586751,
      "grad_norm": 0.8647450904375114,
      "learning_rate": 9.539668695493344e-06,
      "loss": 0.5636,
      "step": 168
    },
    {
      "epoch": 0.5331230283911672,
      "grad_norm": 0.8194507217342969,
      "learning_rate": 9.43747105448054e-06,
      "loss": 0.5801,
      "step": 169
    },
    {
      "epoch": 0.5362776025236593,
      "grad_norm": 0.8810221888411776,
      "learning_rate": 9.335332320084331e-06,
      "loss": 0.5831,
      "step": 170
    },
    {
      "epoch": 0.5394321766561514,
      "grad_norm": 0.8532098279866946,
      "learning_rate": 9.233263188016138e-06,
      "loss": 0.5926,
      "step": 171
    },
    {
      "epoch": 0.5425867507886435,
      "grad_norm": 0.8655880015917973,
      "learning_rate": 9.131274346698797e-06,
      "loss": 0.5928,
      "step": 172
    },
    {
      "epoch": 0.5457413249211357,
      "grad_norm": 0.8402272310825578,
      "learning_rate": 9.029376476147303e-06,
      "loss": 0.6169,
      "step": 173
    },
    {
      "epoch": 0.5488958990536278,
      "grad_norm": 0.8839251076948366,
      "learning_rate": 8.927580246850418e-06,
      "loss": 0.5958,
      "step": 174
    },
    {
      "epoch": 0.5520504731861199,
      "grad_norm": 0.8279611048578284,
      "learning_rate": 8.825896318653294e-06,
      "loss": 0.536,
      "step": 175
    },
    {
      "epoch": 0.555205047318612,
      "grad_norm": 0.8351046319874488,
      "learning_rate": 8.724335339641185e-06,
      "loss": 0.5879,
      "step": 176
    },
    {
      "epoch": 0.5583596214511041,
      "grad_norm": 0.9176384735038398,
      "learning_rate": 8.622907945024418e-06,
      "loss": 0.5302,
      "step": 177
    },
    {
      "epoch": 0.5615141955835962,
      "grad_norm": 0.916200781985982,
      "learning_rate": 8.521624756024706e-06,
      "loss": 0.6105,
      "step": 178
    },
    {
      "epoch": 0.5646687697160884,
      "grad_norm": 0.8535147297061714,
      "learning_rate": 8.420496378762901e-06,
      "loss": 0.5738,
      "step": 179
    },
    {
      "epoch": 0.5678233438485805,
      "grad_norm": 0.8791276583630998,
      "learning_rate": 8.319533403148368e-06,
      "loss": 0.5879,
      "step": 180
    },
    {
      "epoch": 0.5709779179810726,
      "grad_norm": 0.9154660834623038,
      "learning_rate": 8.218746401770021e-06,
      "loss": 0.6025,
      "step": 181
    },
    {
      "epoch": 0.5741324921135647,
      "grad_norm": 0.8756380617683904,
      "learning_rate": 8.118145928789198e-06,
      "loss": 0.6253,
      "step": 182
    },
    {
      "epoch": 0.5772870662460567,
      "grad_norm": 0.917737455549247,
      "learning_rate": 8.017742518834454e-06,
      "loss": 0.5738,
      "step": 183
    },
    {
      "epoch": 0.580441640378549,
      "grad_norm": 0.8961093594700336,
      "learning_rate": 7.917546685898393e-06,
      "loss": 0.5559,
      "step": 184
    },
    {
      "epoch": 0.583596214511041,
      "grad_norm": 0.812457276025284,
      "learning_rate": 7.817568922236683e-06,
      "loss": 0.6034,
      "step": 185
    },
    {
      "epoch": 0.5867507886435331,
      "grad_norm": 0.8211005754359795,
      "learning_rate": 7.717819697269322e-06,
      "loss": 0.5733,
      "step": 186
    },
    {
      "epoch": 0.5899053627760252,
      "grad_norm": 0.8669254450910686,
      "learning_rate": 7.618309456484309e-06,
      "loss": 0.5958,
      "step": 187
    },
    {
      "epoch": 0.5930599369085173,
      "grad_norm": 0.8798776416898837,
      "learning_rate": 7.519048620343825e-06,
      "loss": 0.6186,
      "step": 188
    },
    {
      "epoch": 0.5962145110410094,
      "grad_norm": 0.8228980511474262,
      "learning_rate": 7.42004758319302e-06,
      "loss": 0.5726,
      "step": 189
    },
    {
      "epoch": 0.5993690851735016,
      "grad_norm": 0.8269092669686449,
      "learning_rate": 7.3213167121715514e-06,
      "loss": 0.6118,
      "step": 190
    },
    {
      "epoch": 0.6025236593059937,
      "grad_norm": 0.8799452235440285,
      "learning_rate": 7.222866346127952e-06,
      "loss": 0.5858,
      "step": 191
    },
    {
      "epoch": 0.6056782334384858,
      "grad_norm": 0.832109484028539,
      "learning_rate": 7.124706794536984e-06,
      "loss": 0.5376,
      "step": 192
    },
    {
      "epoch": 0.6088328075709779,
      "grad_norm": 0.76424517564206,
      "learning_rate": 7.026848336420053e-06,
      "loss": 0.5899,
      "step": 193
    },
    {
      "epoch": 0.61198738170347,
      "grad_norm": 0.8711866201466545,
      "learning_rate": 6.929301219268806e-06,
      "loss": 0.5902,
      "step": 194
    },
    {
      "epoch": 0.6151419558359621,
      "grad_norm": 0.8409083620199437,
      "learning_rate": 6.8320756579720545e-06,
      "loss": 0.6158,
      "step": 195
    },
    {
      "epoch": 0.6182965299684543,
      "grad_norm": 0.8709872345876729,
      "learning_rate": 6.735181833746087e-06,
      "loss": 0.6227,
      "step": 196
    },
    {
      "epoch": 0.6214511041009464,
      "grad_norm": 0.8403527161145468,
      "learning_rate": 6.638629893068516e-06,
      "loss": 0.5746,
      "step": 197
    },
    {
      "epoch": 0.6246056782334385,
      "grad_norm": 0.8117860886033016,
      "learning_rate": 6.542429946615774e-06,
      "loss": 0.5705,
      "step": 198
    },
    {
      "epoch": 0.6277602523659306,
      "grad_norm": 0.8876318763943702,
      "learning_rate": 6.446592068204341e-06,
      "loss": 0.6061,
      "step": 199
    },
    {
      "epoch": 0.6309148264984227,
      "grad_norm": 0.8236029320278763,
      "learning_rate": 6.351126293735843e-06,
      "loss": 0.5279,
      "step": 200
    },
    {
      "epoch": 0.6340694006309149,
      "grad_norm": 0.8019477831721104,
      "learning_rate": 6.256042620146119e-06,
      "loss": 0.5359,
      "step": 201
    },
    {
      "epoch": 0.637223974763407,
      "grad_norm": 0.8629687468448992,
      "learning_rate": 6.16135100435836e-06,
      "loss": 0.5537,
      "step": 202
    },
    {
      "epoch": 0.6403785488958991,
      "grad_norm": 0.8767142377162963,
      "learning_rate": 6.06706136224045e-06,
      "loss": 0.5623,
      "step": 203
    },
    {
      "epoch": 0.6435331230283912,
      "grad_norm": 0.8317769124756772,
      "learning_rate": 5.973183567566605e-06,
      "loss": 0.5616,
      "step": 204
    },
    {
      "epoch": 0.6466876971608833,
      "grad_norm": 0.7921663131696103,
      "learning_rate": 5.879727450983412e-06,
      "loss": 0.5383,
      "step": 205
    },
    {
      "epoch": 0.6498422712933754,
      "grad_norm": 0.8224745036093316,
      "learning_rate": 5.786702798980388e-06,
      "loss": 0.5248,
      "step": 206
    },
    {
      "epoch": 0.6529968454258676,
      "grad_norm": 0.8506534156193248,
      "learning_rate": 5.69411935286516e-06,
      "loss": 0.5936,
      "step": 207
    },
    {
      "epoch": 0.6561514195583596,
      "grad_norm": 0.869449194730611,
      "learning_rate": 5.601986807743388e-06,
      "loss": 0.5903,
      "step": 208
    },
    {
      "epoch": 0.6593059936908517,
      "grad_norm": 0.8157896041851987,
      "learning_rate": 5.51031481150352e-06,
      "loss": 0.5365,
      "step": 209
    },
    {
      "epoch": 0.6624605678233438,
      "grad_norm": 0.8922394932071164,
      "learning_rate": 5.419112963806468e-06,
      "loss": 0.6288,
      "step": 210
    },
    {
      "epoch": 0.6656151419558359,
      "grad_norm": 0.8734827173469635,
      "learning_rate": 5.328390815080381e-06,
      "loss": 0.5642,
      "step": 211
    },
    {
      "epoch": 0.668769716088328,
      "grad_norm": 0.7873383069240687,
      "learning_rate": 5.238157865520539e-06,
      "loss": 0.5812,
      "step": 212
    },
    {
      "epoch": 0.6719242902208202,
      "grad_norm": 0.821017499005858,
      "learning_rate": 5.148423564094517e-06,
      "loss": 0.5554,
      "step": 213
    },
    {
      "epoch": 0.6750788643533123,
      "grad_norm": 0.8579785564799017,
      "learning_rate": 5.059197307552698e-06,
      "loss": 0.5562,
      "step": 214
    },
    {
      "epoch": 0.6782334384858044,
      "grad_norm": 0.8093777413061504,
      "learning_rate": 4.970488439444296e-06,
      "loss": 0.5732,
      "step": 215
    },
    {
      "epoch": 0.6813880126182965,
      "grad_norm": 0.8195896366497406,
      "learning_rate": 4.882306249138909e-06,
      "loss": 0.6079,
      "step": 216
    },
    {
      "epoch": 0.6845425867507886,
      "grad_norm": 0.8438423519775164,
      "learning_rate": 4.7946599708537485e-06,
      "loss": 0.581,
      "step": 217
    },
    {
      "epoch": 0.6876971608832808,
      "grad_norm": 0.8111185424551094,
      "learning_rate": 4.707558782686677e-06,
      "loss": 0.5781,
      "step": 218
    },
    {
      "epoch": 0.6908517350157729,
      "grad_norm": 0.8688148042572913,
      "learning_rate": 4.621011805655093e-06,
      "loss": 0.5758,
      "step": 219
    },
    {
      "epoch": 0.694006309148265,
      "grad_norm": 0.8393799796339844,
      "learning_rate": 4.535028102740785e-06,
      "loss": 0.6068,
      "step": 220
    },
    {
      "epoch": 0.6971608832807571,
      "grad_norm": 1.4745526722753934,
      "learning_rate": 4.449616677940904e-06,
      "loss": 0.6088,
      "step": 221
    },
    {
      "epoch": 0.7003154574132492,
      "grad_norm": 0.8249118885243896,
      "learning_rate": 4.364786475325072e-06,
      "loss": 0.5678,
      "step": 222
    },
    {
      "epoch": 0.7034700315457413,
      "grad_norm": 0.8326911453838707,
      "learning_rate": 4.280546378098792e-06,
      "loss": 0.5859,
      "step": 223
    },
    {
      "epoch": 0.7066246056782335,
      "grad_norm": 0.8615895547137272,
      "learning_rate": 4.196905207673201e-06,
      "loss": 0.5963,
      "step": 224
    },
    {
      "epoch": 0.7097791798107256,
      "grad_norm": 0.7949292154344004,
      "learning_rate": 4.113871722741337e-06,
      "loss": 0.5913,
      "step": 225
    },
    {
      "epoch": 0.7129337539432177,
      "grad_norm": 0.7850637865948522,
      "learning_rate": 4.031454618360945e-06,
      "loss": 0.5201,
      "step": 226
    },
    {
      "epoch": 0.7160883280757098,
      "grad_norm": 0.8075500308603617,
      "learning_rate": 3.949662525043935e-06,
      "loss": 0.5436,
      "step": 227
    },
    {
      "epoch": 0.7192429022082019,
      "grad_norm": 0.7912361660788771,
      "learning_rate": 3.868504007852641e-06,
      "loss": 0.5396,
      "step": 228
    },
    {
      "epoch": 0.722397476340694,
      "grad_norm": 0.8218250886836489,
      "learning_rate": 3.7879875655029018e-06,
      "loss": 0.5201,
      "step": 229
    },
    {
      "epoch": 0.7255520504731862,
      "grad_norm": 0.8996725368992923,
      "learning_rate": 3.7081216294740773e-06,
      "loss": 0.5541,
      "step": 230
    },
    {
      "epoch": 0.7287066246056783,
      "grad_norm": 0.7756090139327416,
      "learning_rate": 3.628914563126156e-06,
      "loss": 0.533,
      "step": 231
    },
    {
      "epoch": 0.7318611987381703,
      "grad_norm": 0.8934419101657091,
      "learning_rate": 3.5503746608239487e-06,
      "loss": 0.551,
      "step": 232
    },
    {
      "epoch": 0.7350157728706624,
      "grad_norm": 0.8040538796260475,
      "learning_rate": 3.472510147068515e-06,
      "loss": 0.5845,
      "step": 233
    },
    {
      "epoch": 0.7381703470031545,
      "grad_norm": 0.8488193304095677,
      "learning_rate": 3.3953291756359354e-06,
      "loss": 0.5469,
      "step": 234
    },
    {
      "epoch": 0.7413249211356467,
      "grad_norm": 0.7860770025071445,
      "learning_rate": 3.3188398287234504e-06,
      "loss": 0.5402,
      "step": 235
    },
    {
      "epoch": 0.7444794952681388,
      "grad_norm": 0.8264849562344951,
      "learning_rate": 3.243050116103128e-06,
      "loss": 0.599,
      "step": 236
    },
    {
      "epoch": 0.7476340694006309,
      "grad_norm": 0.8675996932260254,
      "learning_rate": 3.1679679742830806e-06,
      "loss": 0.6017,
      "step": 237
    },
    {
      "epoch": 0.750788643533123,
      "grad_norm": 0.8517406499795336,
      "learning_rate": 3.0936012656763937e-06,
      "loss": 0.5578,
      "step": 238
    },
    {
      "epoch": 0.7539432176656151,
      "grad_norm": 0.8352092538615361,
      "learning_rate": 3.019957777777788e-06,
      "loss": 0.5371,
      "step": 239
    },
    {
      "epoch": 0.7570977917981072,
      "grad_norm": 0.8190748601674349,
      "learning_rate": 2.9470452223481206e-06,
      "loss": 0.56,
      "step": 240
    },
    {
      "epoch": 0.7602523659305994,
      "grad_norm": 0.7759478264593801,
      "learning_rate": 2.8748712346068464e-06,
      "loss": 0.4696,
      "step": 241
    },
    {
      "epoch": 0.7634069400630915,
      "grad_norm": 0.8161144576091283,
      "learning_rate": 2.8034433724324716e-06,
      "loss": 0.5462,
      "step": 242
    },
    {
      "epoch": 0.7665615141955836,
      "grad_norm": 0.8232078571758132,
      "learning_rate": 2.7327691155710978e-06,
      "loss": 0.5989,
      "step": 243
    },
    {
      "epoch": 0.7697160883280757,
      "grad_norm": 0.7882461920386135,
      "learning_rate": 2.6628558648531845e-06,
      "loss": 0.5168,
      "step": 244
    },
    {
      "epoch": 0.7728706624605678,
      "grad_norm": 0.7994117069152421,
      "learning_rate": 2.593710941418537e-06,
      "loss": 0.5426,
      "step": 245
    },
    {
      "epoch": 0.7760252365930599,
      "grad_norm": 0.8212592992518652,
      "learning_rate": 2.525341585949662e-06,
      "loss": 0.5956,
      "step": 246
    },
    {
      "epoch": 0.7791798107255521,
      "grad_norm": 0.8078436558371133,
      "learning_rate": 2.4577549579135318e-06,
      "loss": 0.591,
      "step": 247
    },
    {
      "epoch": 0.7823343848580442,
      "grad_norm": 0.837764473781124,
      "learning_rate": 2.3909581348118803e-06,
      "loss": 0.5895,
      "step": 248
    },
    {
      "epoch": 0.7854889589905363,
      "grad_norm": 0.8400849871016951,
      "learning_rate": 2.324958111440051e-06,
      "loss": 0.5807,
      "step": 249
    },
    {
      "epoch": 0.7886435331230284,
      "grad_norm": 0.8112523523941368,
      "learning_rate": 2.259761799154516e-06,
      "loss": 0.5577,
      "step": 250
    },
    {
      "epoch": 0.7917981072555205,
      "grad_norm": 0.826943131758786,
      "learning_rate": 2.195376025149156e-06,
      "loss": 0.6393,
      "step": 251
    },
    {
      "epoch": 0.7949526813880127,
      "grad_norm": 0.8295057481187702,
      "learning_rate": 2.1318075317403152e-06,
      "loss": 0.5867,
      "step": 252
    },
    {
      "epoch": 0.7981072555205048,
      "grad_norm": 0.7840299589315893,
      "learning_rate": 2.069062975660765e-06,
      "loss": 0.5626,
      "step": 253
    },
    {
      "epoch": 0.8012618296529969,
      "grad_norm": 0.852618790587983,
      "learning_rate": 2.0071489273626376e-06,
      "loss": 0.5209,
      "step": 254
    },
    {
      "epoch": 0.804416403785489,
      "grad_norm": 0.846893541681965,
      "learning_rate": 1.946071870329377e-06,
      "loss": 0.5538,
      "step": 255
    },
    {
      "epoch": 0.807570977917981,
      "grad_norm": 0.8190397000182341,
      "learning_rate": 1.885838200396808e-06,
      "loss": 0.5526,
      "step": 256
    },
    {
      "epoch": 0.8107255520504731,
      "grad_norm": 0.8449333521353117,
      "learning_rate": 1.826454225083375e-06,
      "loss": 0.5372,
      "step": 257
    },
    {
      "epoch": 0.8138801261829653,
      "grad_norm": 0.8692727325104254,
      "learning_rate": 1.7679261629296408e-06,
      "loss": 0.591,
      "step": 258
    },
    {
      "epoch": 0.8170347003154574,
      "grad_norm": 0.7945659392153616,
      "learning_rate": 1.7102601428470988e-06,
      "loss": 0.5311,
      "step": 259
    },
    {
      "epoch": 0.8201892744479495,
      "grad_norm": 0.7568950500161471,
      "learning_rate": 1.6534622034763558e-06,
      "loss": 0.586,
      "step": 260
    },
    {
      "epoch": 0.8233438485804416,
      "grad_norm": 0.8223530189999473,
      "learning_rate": 1.5975382925547966e-06,
      "loss": 0.5693,
      "step": 261
    },
    {
      "epoch": 0.8264984227129337,
      "grad_norm": 0.8646628729285558,
      "learning_rate": 1.5424942662937436e-06,
      "loss": 0.6563,
      "step": 262
    },
    {
      "epoch": 0.8296529968454258,
      "grad_norm": 0.8315972425392338,
      "learning_rate": 1.4883358887652044e-06,
      "loss": 0.5827,
      "step": 263
    },
    {
      "epoch": 0.832807570977918,
      "grad_norm": 0.8244377224778475,
      "learning_rate": 1.4350688312982864e-06,
      "loss": 0.5223,
      "step": 264
    },
    {
      "epoch": 0.8359621451104101,
      "grad_norm": 0.803096744856657,
      "learning_rate": 1.3826986718852952e-06,
      "loss": 0.5349,
      "step": 265
    },
    {
      "epoch": 0.8391167192429022,
      "grad_norm": 0.7910224383010122,
      "learning_rate": 1.3312308945976348e-06,
      "loss": 0.5097,
      "step": 266
    },
    {
      "epoch": 0.8422712933753943,
      "grad_norm": 0.8529170986896591,
      "learning_rate": 1.2806708890115138e-06,
      "loss": 0.5757,
      "step": 267
    },
    {
      "epoch": 0.8454258675078864,
      "grad_norm": 0.8218380739057664,
      "learning_rate": 1.2310239496435749e-06,
      "loss": 0.5793,
      "step": 268
    },
    {
      "epoch": 0.8485804416403786,
      "grad_norm": 0.8505882696555029,
      "learning_rate": 1.1822952753964667e-06,
      "loss": 0.529,
      "step": 269
    },
    {
      "epoch": 0.8517350157728707,
      "grad_norm": 0.809596233238013,
      "learning_rate": 1.134489969014414e-06,
      "loss": 0.55,
      "step": 270
    },
    {
      "epoch": 0.8548895899053628,
      "grad_norm": 0.8259346526159871,
      "learning_rate": 1.087613036548888e-06,
      "loss": 0.5416,
      "step": 271
    },
    {
      "epoch": 0.8580441640378549,
      "grad_norm": 0.8339916120638338,
      "learning_rate": 1.0416693868343796e-06,
      "loss": 0.5636,
      "step": 272
    },
    {
      "epoch": 0.861198738170347,
      "grad_norm": 0.821833579161445,
      "learning_rate": 9.966638309743481e-07,
      "loss": 0.5428,
      "step": 273
    },
    {
      "epoch": 0.8643533123028391,
      "grad_norm": 0.8509914699813607,
      "learning_rate": 9.52601081837431e-07,
      "loss": 0.5226,
      "step": 274
    },
    {
      "epoch": 0.8675078864353313,
      "grad_norm": 0.8098580797480884,
      "learning_rate": 9.094857535639157e-07,
      "loss": 0.5654,
      "step": 275
    },
    {
      "epoch": 0.8706624605678234,
      "grad_norm": 0.823936317807482,
      "learning_rate": 8.673223610825532e-07,
      "loss": 0.5739,
      "step": 276
    },
    {
      "epoch": 0.8738170347003155,
      "grad_norm": 0.79206886679379,
      "learning_rate": 8.261153196377814e-07,
      "loss": 0.5231,
      "step": 277
    },
    {
      "epoch": 0.8769716088328076,
      "grad_norm": 0.8299317093133516,
      "learning_rate": 7.858689443273548e-07,
      "loss": 0.5804,
      "step": 278
    },
    {
      "epoch": 0.8801261829652997,
      "grad_norm": 0.8482278502549674,
      "learning_rate": 7.465874496504944e-07,
      "loss": 0.5833,
      "step": 279
    },
    {
      "epoch": 0.8832807570977917,
      "grad_norm": 0.8332203588132586,
      "learning_rate": 7.082749490665353e-07,
      "loss": 0.5574,
      "step": 280
    },
    {
      "epoch": 0.886435331230284,
      "grad_norm": 0.7768320276749398,
      "learning_rate": 6.709354545641989e-07,
      "loss": 0.5833,
      "step": 281
    },
    {
      "epoch": 0.889589905362776,
      "grad_norm": 0.8048346461429382,
      "learning_rate": 6.345728762414504e-07,
      "loss": 0.5233,
      "step": 282
    },
    {
      "epoch": 0.8927444794952681,
      "grad_norm": 0.7669936767639267,
      "learning_rate": 5.99191021896055e-07,
      "loss": 0.5593,
      "step": 283
    },
    {
      "epoch": 0.8958990536277602,
      "grad_norm": 0.8258852856592097,
      "learning_rate": 5.647935966268225e-07,
      "loss": 0.5538,
      "step": 284
    },
    {
      "epoch": 0.8990536277602523,
      "grad_norm": 0.8249627383440178,
      "learning_rate": 5.313842024456306e-07,
      "loss": 0.5631,
      "step": 285
    },
    {
      "epoch": 0.9022082018927445,
      "grad_norm": 0.8104101897029812,
      "learning_rate": 4.98966337900224e-07,
      "loss": 0.5825,
      "step": 286
    },
    {
      "epoch": 0.9053627760252366,
      "grad_norm": 0.8184087281912953,
      "learning_rate": 4.6754339770785474e-07,
      "loss": 0.5026,
      "step": 287
    },
    {
      "epoch": 0.9085173501577287,
      "grad_norm": 0.8214249107876357,
      "learning_rate": 4.3711867239980335e-07,
      "loss": 0.5818,
      "step": 288
    },
    {
      "epoch": 0.9116719242902208,
      "grad_norm": 0.8553341089665368,
      "learning_rate": 4.076953479767964e-07,
      "loss": 0.513,
      "step": 289
    },
    {
      "epoch": 0.9148264984227129,
      "grad_norm": 0.8128019036411953,
      "learning_rate": 3.792765055753755e-07,
      "loss": 0.5688,
      "step": 290
    },
    {
      "epoch": 0.917981072555205,
      "grad_norm": 0.827848343095642,
      "learning_rate": 3.5186512114525283e-07,
      "loss": 0.5218,
      "step": 291
    },
    {
      "epoch": 0.9211356466876972,
      "grad_norm": 0.8283913486224622,
      "learning_rate": 3.25464065137675e-07,
      "loss": 0.5411,
      "step": 292
    },
    {
      "epoch": 0.9242902208201893,
      "grad_norm": 0.8030335272327884,
      "learning_rate": 3.0007610220483927e-07,
      "loss": 0.5821,
      "step": 293
    },
    {
      "epoch": 0.9274447949526814,
      "grad_norm": 0.816940489369376,
      "learning_rate": 2.757038909103793e-07,
      "loss": 0.5875,
      "step": 294
    },
    {
      "epoch": 0.9305993690851735,
      "grad_norm": 0.7647918372503908,
      "learning_rate": 2.523499834509724e-07,
      "loss": 0.4797,
      "step": 295
    },
    {
      "epoch": 0.9337539432176656,
      "grad_norm": 0.8320367121449735,
      "learning_rate": 2.3001682538908333e-07,
      "loss": 0.5717,
      "step": 296
    },
    {
      "epoch": 0.9369085173501577,
      "grad_norm": 0.7576397010342387,
      "learning_rate": 2.0870675539686024e-07,
      "loss": 0.552,
      "step": 297
    },
    {
      "epoch": 0.9400630914826499,
      "grad_norm": 0.8191027416746758,
      "learning_rate": 1.884220050112462e-07,
      "loss": 0.5231,
      "step": 298
    },
    {
      "epoch": 0.943217665615142,
      "grad_norm": 0.7900863476041063,
      "learning_rate": 1.691646984002937e-07,
      "loss": 0.532,
      "step": 299
    },
    {
      "epoch": 0.9463722397476341,
      "grad_norm": 0.8149273736896135,
      "learning_rate": 1.5093685214072173e-07,
      "loss": 0.5444,
      "step": 300
    },
    {
      "epoch": 0.9495268138801262,
      "grad_norm": 0.8395199500617847,
      "learning_rate": 1.3374037500675452e-07,
      "loss": 0.533,
      "step": 301
    },
    {
      "epoch": 0.9526813880126183,
      "grad_norm": 0.7985447289990664,
      "learning_rate": 1.1757706777023592e-07,
      "loss": 0.5743,
      "step": 302
    },
    {
      "epoch": 0.9558359621451105,
      "grad_norm": 0.8055531754452964,
      "learning_rate": 1.024486230120525e-07,
      "loss": 0.5719,
      "step": 303
    },
    {
      "epoch": 0.9589905362776026,
      "grad_norm": 0.8325722456393257,
      "learning_rate": 8.835662494489638e-08,
      "loss": 0.579,
      "step": 304
    },
    {
      "epoch": 0.9621451104100947,
      "grad_norm": 0.813739667712494,
      "learning_rate": 7.530254924736691e-08,
      "loss": 0.5423,
      "step": 305
    },
    {
      "epoch": 0.9652996845425867,
      "grad_norm": 0.857945459541129,
      "learning_rate": 6.32877629094475e-08,
      "loss": 0.5517,
      "step": 306
    },
    {
      "epoch": 0.9684542586750788,
      "grad_norm": 0.8120861613245415,
      "learning_rate": 5.231352408934687e-08,
      "loss": 0.6026,
      "step": 307
    },
    {
      "epoch": 0.9716088328075709,
      "grad_norm": 0.8443466714196998,
      "learning_rate": 4.2380981981759994e-08,
      "loss": 0.5325,
      "step": 308
    },
    {
      "epoch": 0.9747634069400631,
      "grad_norm": 0.8304117125567168,
      "learning_rate": 3.349117669751767e-08,
      "loss": 0.5639,
      "step": 309
    },
    {
      "epoch": 0.9779179810725552,
      "grad_norm": 0.8363387160532579,
      "learning_rate": 2.5645039154675867e-08,
      "loss": 0.6035,
      "step": 310
    },
    {
      "epoch": 0.9810725552050473,
      "grad_norm": 0.8114375994963179,
      "learning_rate": 1.8843390981024835e-08,
      "loss": 0.6084,
      "step": 311
    },
    {
      "epoch": 0.9842271293375394,
      "grad_norm": 0.777094131602412,
      "learning_rate": 1.3086944428060132e-08,
      "loss": 0.5899,
      "step": 312
    },
    {
      "epoch": 0.9873817034700315,
      "grad_norm": 0.8176853933317341,
      "learning_rate": 8.376302296387862e-09,
      "loss": 0.5139,
      "step": 313
    },
    {
      "epoch": 0.9905362776025236,
      "grad_norm": 0.8131581144368385,
      "learning_rate": 4.711957872606254e-09,
      "loss": 0.5187,
      "step": 314
    },
    {
      "epoch": 0.9936908517350158,
      "grad_norm": 0.8251830982107549,
      "learning_rate": 2.0942948776481175e-09,
      "loss": 0.5269,
      "step": 315
    },
    {
      "epoch": 0.9968454258675079,
      "grad_norm": 0.7558292007438561,
      "learning_rate": 5.23587426601857e-10,
      "loss": 0.5618,
      "step": 316
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.788664005350259,
      "learning_rate": 0.0,
      "loss": 0.5037,
      "step": 317
    },
    {
      "epoch": 1.0,
      "step": 317,
      "total_flos": 112643251601408.0,
      "train_loss": 0.635259664491148,
      "train_runtime": 5154.3577,
      "train_samples_per_second": 7.856,
      "train_steps_per_second": 0.062
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 317,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 112643251601408.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
